# 제8장: 선형 회귀 입문

## 목차
- 8.1 직선 적합, 잔차, 상관관계
- 8.2 최소제곱 회귀
- 8.3 선형 회귀에서의 이상치 유형
- 8.4 선형 회귀에 대한 추론

---

선형 회귀는 매우 강력한 통계 기법이다. 많은 사람들이 뉴스에서 산점도 위에 직선이 덧씌워진 것을 보며 회귀에 대해 어느 정도 친숙함을 갖고 있다. 선형 모형은 예측에 사용되거나 두 수치형 변수 사이에 선형 관계가 있는지 평가하는 데 사용될 수 있다.

---

## 8.1 직선 적합, 잔차, 상관관계

직선 적합 과정에 대해 깊이 생각하는 것이 도움이 된다. 이 절에서는 선형 모형의 형태를 정의하고, 좋은 적합이란 무엇인지에 대한 기준을 탐구하며, 상관관계라는 새로운 통계량을 소개한다.

### 8.1.1 데이터에 직선 적합하기

두 변수의 관계가 직선으로 완벽하게 모형화될 수 있는 경우를 고려해보자. 직선의 방정식은 다음과 같다:

$$y = 5 + 64.96x$$

완벽한 선형 관계가 의미하는 바를 생각해보면, x의 값만 알면 y의 정확한 값을 알 수 있다. 이것은 거의 모든 자연 과정에서 비현실적이다. 예를 들어, 가족 소득(x)을 알면 대학이 예비 학생에게 제공할 재정 지원(y)에 대한 유용한 정보를 얻을 수 있다. 그러나 가족 재정 외에도 다른 요인들이 재정 지원에 영향을 미치므로 예측은 완벽과는 거리가 멀 것이다.

**선형 회귀**는 두 변수 x와 y 사이의 관계가 약간의 오차를 가진 직선으로 모형화될 수 있는 데이터에 직선을 적합하는 통계적 방법이다:

$$y = \beta_0 + \beta_1 x + \varepsilon$$

값 β₀와 β₁은 모형의 **모수**(parameters)를 나타내며(β는 그리스 문자 베타), 오차는 ε(그리스 문자 엡실론)로 나타낸다. 모수는 데이터를 사용하여 추정되며, 점추정치를 b₀와 b₁로 쓴다. x를 사용하여 y를 예측할 때, x를 보통 **설명변수** 또는 **예측변수**라고 부르고, y를 **반응변수**라고 부른다.

모든 데이터가 직선 위에 완벽하게 놓이는 것은 드문 일이다. 대신, 데이터가 점들의 구름으로 나타나는 것이 더 일반적이다. 데이터는 직선 주위에 떨어지지만, 관측치 중 어느 것도 정확히 직선 위에 놓이지 않는다.

```python
# 그림 8.2: 다양한 선형 관계 강도 시각화
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(42)
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# 강한 음의 관계
x1 = np.linspace(0, 10, 50)
y1 = 50 - 4*x1 + np.random.normal(0, 2, 50)
axes[0].scatter(x1, y1, alpha=0.7, s=50)
z1 = np.polyfit(x1, y1, 1)
axes[0].plot(x1, np.polyval(z1, x1), 'r-', linewidth=2)
axes[0].set_title('강한 음의 관계', fontsize=12)
axes[0].set_xlabel('x')
axes[0].set_ylabel('y')

# 중간 양의 관계
x2 = np.linspace(0, 10, 50)
y2 = 10 + 3*x2 + np.random.normal(0, 6, 50)
axes[1].scatter(x2, y2, alpha=0.7, s=50)
z2 = np.polyfit(x2, y2, 1)
axes[1].plot(x2, np.polyval(z2, x2), 'r-', linewidth=2)
axes[1].set_title('중간 양의 관계', fontsize=12)
axes[1].set_xlabel('x')
axes[1].set_ylabel('y')

# 약한 음의 관계
x3 = np.linspace(0, 10, 50)
y3 = 30 - 0.5*x3 + np.random.normal(0, 8, 50)
axes[2].scatter(x3, y3, alpha=0.7, s=50)
z3 = np.polyfit(x3, y3, 1)
axes[2].plot(x3, np.polyval(z3, x3), 'r-', linewidth=2)
axes[2].set_title('약한 음의 관계', fontsize=12)
axes[2].set_xlabel('x')
axes[2].set_ylabel('y')

plt.tight_layout()
plt.savefig('fig8_2_relationships.png', dpi=150, bbox_inches='tight')
plt.show()
```

### 8.1.2 주머니쥐 머리 길이를 예측하기 위한 선형 회귀 사용

데이터는 호주의 주머니쥐(possum)에 대해 수집되었으며, 각 관측치에서 주머니쥐의 전체 길이와 머리 길이가 측정되었다. 주머니쥐의 머리 길이 측정은 더 어렵기 때문에, 동물의 전체 길이를 기반으로 머리 길이를 예측하는 것이 유용할 것이다.

산점도는 두 수치형 변수 사이의 관계를 보여주며, 산점도에서 나타난 추세를 설명하기 위해 직선을 추가할 수 있다. 직선 적합에서 각 관측치(xᵢ, yᵢ)에 대해 직선 위의 예측값은 다음과 같다:

$$\hat{y}_i = b_0 + b_1 x_i$$

**ŷ**는 "y-hat"으로 읽으며, 이 표기는 y의 예측값을 나타낸다. "^" 기호는 예측값과 실제 관측값을 구별하는 데 사용된다.

#### 예제 8.1

**문제**: 주머니쥐 데이터에 대해 적합한 직선 방정식은 다음과 같다:

$$\hat{y} = 41 + 0.59x$$

이 정보를 사용하여 전체 길이가 80cm인 주머니쥐의 머리 길이를 예측하라.

**풀이**: 방정식에 x = 80을 대입한다:

$$\hat{y} = 41 + 0.59 \times 80 = 41 + 47.2 = 88.2 \text{ mm}$$

전체 길이가 80cm인 주머니쥐의 머리 길이는 약 88.2mm로 예측된다.

---

```python
# 그림 8.3: 주머니쥐 데이터와 회귀선
import matplotlib.pyplot as plt
import numpy as np

# 주머니쥐 데이터 (예시)
np.random.seed(123)
total_length = np.array([75, 77, 80, 82, 85, 87, 89, 91, 93, 95,
                         78, 81, 84, 86, 88, 90, 92, 94, 76, 79,
                         83, 85, 87, 89, 91, 93, 77, 80, 82, 84,
                         86, 88, 90, 92, 94, 96, 78, 81, 83, 85])

head_length = 41 + 0.59 * total_length + np.random.normal(0, 2.5, len(total_length))

plt.figure(figsize=(10, 7))
plt.scatter(total_length, head_length, alpha=0.6, s=60, label='관측 데이터')

# 회귀선
x_line = np.linspace(total_length.min()-2, total_length.max()+2, 100)
y_line = 41 + 0.59 * x_line
plt.plot(x_line, y_line, 'r-', linewidth=2, label='$\\hat{y} = 41 + 0.59x$')

# 특정 예측 표시
plt.scatter([80], [88.2], color='green', s=150, zorder=5, marker='*', 
            label='예측: x=80 → ŷ=88.2')
plt.axvline(x=80, color='green', linestyle='--', alpha=0.5)
plt.axhline(y=88.2, color='green', linestyle='--', alpha=0.5)

plt.xlabel('전체 길이 (cm)', fontsize=12)
plt.ylabel('머리 길이 (mm)', fontsize=12)
plt.title('주머니쥐 전체 길이와 머리 길이', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('fig8_3_possum.png', dpi=150, bbox_inches='tight')
plt.show()
```

---

### 8.1.3 잔차

**잔차**(residuals)는 관측값과 선형 모형의 예측값 사이의 차이이다. 각 관측치의 잔차는 다음과 같이 계산된다:

$$e_i = y_i - \hat{y}_i$$

관측치가 직선 위에 있으면, 잔차는 0이 된다. 관측치가 직선 위에 있으면 잔차는 양수이고, 직선 아래에 있으면 잔차는 음수이다.

> **잔차: 관측값과 예측값의 차이**
>
> 잔차는 직선에서 관측값이 얼마나 멀리 떨어져 있는지와 그 방향을 나타낸다.
>
> $$e_i = y_i - \hat{y}_i$$
>
> 양의 잔차는 직선이 **과소예측**했음을, 음의 잔차는 **과대예측**했음을 의미한다.

---

#### 예제 8.2

**문제**: 한 주머니쥐의 머리 길이가 86.7mm이고 전체 길이가 84cm이다. 이 주머니쥐에 대한 잔차는 얼마인가?

**풀이**: 먼저 모형으로 예측값을 계산한다:

$$\hat{y} = 41 + 0.59 \times 84 = 41 + 49.56 = 90.56 \text{ mm}$$

잔차는:

$$e = y - \hat{y} = 86.7 - 90.56 = -3.86 \text{ mm}$$

음의 잔차는 모형이 이 주머니쥐의 머리 길이를 과대예측했음을 의미한다.

---

#### 안내 실습 8.3

**문제**: 한 주머니쥐의 머리 길이가 95.4mm이고 전체 길이가 85cm이다. 이 주머니쥐에 대한 잔차는 얼마인가?

**풀이**: 예측값: $\hat{y} = 41 + 0.59 \times 85 = 91.15$ mm

잔차: $e = 95.4 - 91.15 = 4.25$ mm

양의 잔차이므로, 실제 머리 길이가 예측값보다 크다.

---

#### 잔차 그림

잔차 그림(residual plot)은 잔차와 설명변수 x의 관계를 보여주는 산점도이다. 잔차 그림은 선형 모형이 데이터에 적절한지 평가하는 데 유용하다.

```python
# 그림 8.6: 잔차 그림
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(123)
total_length = np.array([75, 77, 80, 82, 85, 87, 89, 91, 93, 95,
                         78, 81, 84, 86, 88, 90, 92, 94, 76, 79])
head_length = 41 + 0.59 * total_length + np.random.normal(0, 2.5, len(total_length))

# 예측값과 잔차 계산
predicted = 41 + 0.59 * total_length
residuals = head_length - predicted

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 원본 산점도
axes[0].scatter(total_length, head_length, alpha=0.7, s=60)
x_line = np.linspace(70, 100, 100)
axes[0].plot(x_line, 41 + 0.59*x_line, 'r-', linewidth=2)
axes[0].set_xlabel('전체 길이 (cm)', fontsize=11)
axes[0].set_ylabel('머리 길이 (mm)', fontsize=11)
axes[0].set_title('산점도와 회귀선', fontsize=12)
axes[0].grid(True, alpha=0.3)

# 잔차 그림
axes[1].scatter(total_length, residuals, alpha=0.7, s=60)
axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)
axes[1].set_xlabel('전체 길이 (cm)', fontsize=11)
axes[1].set_ylabel('잔차 (mm)', fontsize=11)
axes[1].set_title('잔차 그림', fontsize=12)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('fig8_6_residual_plot.png', dpi=150, bbox_inches='tight')
plt.show()
```

---

### 8.1.4 상관관계로 선형 관계 설명하기

강한 선형 관계를 가진 그림과 매우 약한 선형 관계를 가진 그림이 있다. 통계량으로 이러한 선형 관계의 강도를 정량화할 수 있다면 유용할 것이다.

> **상관관계: 선형 관계의 강도**
>
> **상관관계**(correlation)는 항상 -1과 1 사이의 값을 가지며, 두 변수 사이의 선형 관계의 강도를 설명한다. 상관관계를 R로 표기한다.

상관계수는 공식을 사용하여 계산할 수 있다:

$$R = \frac{1}{n-1} \sum_{i=1}^{n} \frac{x_i - \bar{x}}{s_x} \cdot \frac{y_i - \bar{y}}{s_y}$$

여기서 $\bar{x}$, $\bar{y}$는 표본평균이고, $s_x$, $s_y$는 표본표준편차이다.

---

#### 상관계수 해석 지침

| 상관계수 R | 해석 |
|-----------|------|
| R = 1 | 완벽한 양의 선형 관계 |
| 0.7 ≤ R < 1 | 강한 양의 선형 관계 |
| 0.3 ≤ R < 0.7 | 중간 정도의 양의 선형 관계 |
| 0 < R < 0.3 | 약한 양의 선형 관계 |
| R = 0 | 선형 관계 없음 |
| -0.3 < R < 0 | 약한 음의 선형 관계 |
| -0.7 < R ≤ -0.3 | 중간 정도의 음의 선형 관계 |
| -1 < R ≤ -0.7 | 강한 음의 선형 관계 |
| R = -1 | 완벽한 음의 선형 관계 |

```python
# 그림 8.9: 다양한 상관계수 시각화
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(42)

def generate_correlated_data(r, n=50):
    """특정 상관계수를 갖는 데이터 생성"""
    x = np.random.normal(0, 1, n)
    y = r * x + np.sqrt(1 - r**2) * np.random.normal(0, 1, n)
    return x, y

correlations = [1.0, 0.98, 0.69, 0.33, 0.08, -0.64, -0.92, -1.0]
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

for i, r in enumerate(correlations):
    if abs(r) == 1:
        x = np.linspace(-2, 2, 50)
        y = r * x
    else:
        x, y = generate_correlated_data(r)
    
    axes[i].scatter(x, y, alpha=0.6, s=30)
    z = np.polyfit(x, y, 1)
    x_line = np.linspace(x.min(), x.max(), 100)
    axes[i].plot(x_line, np.polyval(z, x_line), 'r-', linewidth=2)
    axes[i].set_title(f'R = {r:.2f}', fontsize=12)
    axes[i].grid(True, alpha=0.3)

plt.suptitle('다양한 상관계수를 가진 산점도', fontsize=14, y=1.02)
plt.tight_layout()
plt.savefig('fig8_9_correlations.png', dpi=150, bbox_inches='tight')
plt.show()
```

---

#### 안내 실습 8.5

**문제**: 그림에서 양의 상관관계 또는 음의 상관관계를 나타내는지 식별하라. 또한 상관관계가 약한지, 보통인지, 강한지 결정하라.

**풀이**: 두 변수의 추세를 살펴본다:
- 상향 추세면 양의 상관관계
- 하향 추세면 음의 상관관계
- 점들이 직선 주위에 얼마나 밀집해 있는지에 따라 강도 결정

---

> **새로운 시각: 상관관계 ≠ 인과관계**
>
> 통계학에서 가장 중요한 원칙 중 하나는 **상관관계가 인과관계를 의미하지 않는다**는 것이다. 
>
> 두 변수 사이에 강한 상관관계가 있다고 해서 한 변수가 다른 변수를 "유발"한다고 결론 내릴 수 없다. 예를 들어:
> - 아이스크림 판매량과 익사 사고 사이에 양의 상관관계가 있다 (둘 다 여름에 증가)
> - 하지만 아이스크림이 익사를 유발하는 것은 아니다!
> - 공통 원인(더운 날씨)이 두 변수에 영향을 미친다
>
> 인과관계를 확립하려면 무작위 대조 실험이 필요하다.

---

## 8.1절 연습문제

### 연습문제 8.1: 잔차 시각화

**문제**: 산점도에 회귀선이 중첩되어 있다. 잔차 그림(잔차 대 x)을 구성한다면 어떻게 보일지 설명하라.

**풀이**: 잔차 그림에서:
- 점들이 0 주위에 무작위로 흩어져 있으면 선형 모형이 적절
- 패턴(곡선, 팬 모양 등)이 보이면 모형이 부적절할 수 있음

---

### 연습문제 8.3: 신체 측정, 파트 I

**문제**: 신체 측정 데이터에서 어깨 둘레(cm)로부터 키(cm)를 예측하는 모형을 적합했다. 아래의 잔차 그림은 어깨 둘레를 이용하여 키를 예측하기 위한 선형 모형을 적합하는 것에 대해 무엇을 나타내는가?

**풀이**: 잔차 그림에서 점들이 0 주위에 대칭적으로 무작위하게 흩어져 있으면, 선형 모형이 적절하다고 판단할 수 있다. 팬 모양이나 곡선 패턴이 없어야 한다.

---

### 연습문제 8.5: 고양이, 파트 I

**문제**: 산점도는 144마리 가정용 고양이의 심장 무게(g)와 체중(kg) 사이의 관계를 보여준다. 체중을 사용하여 심장 무게를 예측하는 최소제곱 회귀선도 표시되어 있다.

(a) 체중과 심장 무게 사이의 관계를 설명하라.
(b) 데이터가 최소제곱 회귀선을 적합하기 위한 조건을 만족하는지 설명하라.

**풀이**:

(a) 체중과 심장 무게 사이에 양의 중간 정도에서 강한 선형 관계가 있는 것으로 보인다. 체중이 증가함에 따라 심장 무게도 증가하는 경향이 있다.

(b) 선형성, 정규 잔차, 등분산성, 독립 관측치의 네 가지 조건을 확인해야 한다.

---

### 연습문제 8.7: 상관관계 매칭

**문제**: 각 상관관계를 해당 산점도에 매칭하라.

(a) r = 0.7    (b) r = 0.45    (c) r = 0.06    (d) r = 0.92

**풀이 가이드**:
- r = 0.92: 가장 강한 양의 관계, 점들이 직선에 가장 가깝게 밀집
- r = 0.7: 강한 양의 관계
- r = 0.45: 중간 정도의 양의 관계
- r = 0.06: 거의 선형 관계 없음, 무작위로 흩어짐

---

## 8.2 최소제곱 회귀

### 8.2.1 엘름허스트 대학 신입생에 대한 장학금

엘름허스트 대학의 50명 신입생에 대해 가족 소득과 대학에서 받은 장학금 금액이 수집되었다. 그림 8.11은 이 두 변수의 산점도와 두 개의 직선을 보여준다. 이 데이터를 사용하여 학생의 가족 소득을 기반으로 예상 장학금을 예측할 수 있다.

### 8.2.2 최적의 직선을 찾기 위한 객관적 측도

"최적"이 무엇을 의미하는지 생각하는 것으로 시작한다. 수학적으로, 작은 잔차를 가진 직선을 원한다. 

**잔차 절댓값의 합 최소화**:
$$|e_1| + |e_2| + \cdots + |e_n|$$

**잔차 제곱합 최소화** (더 일반적):
$$e_1^2 + e_2^2 + \cdots + e_n^2$$

이 **최소제곱 기준**을 최소화하는 직선을 **최소제곱선**(least squares line)이라고 한다.

잔차를 제곱하는 이유:
1. 가장 일반적으로 사용되는 방법
2. 통계 소프트웨어에서 널리 지원
3. 큰 잔차에 더 큰 패널티 부여 (2배 큰 잔차는 2배 이상 나쁨)

---

### 8.2.3 최소제곱선을 위한 조건

최소제곱선을 적합할 때, 일반적으로 다음 조건들이 필요하다:

> **최소제곱 회귀의 4가지 조건**
>
> 1. **선형성**(Linearity): 데이터가 선형 추세를 보여야 한다.
> 2. **거의 정규인 잔차**(Nearly normal residuals): 잔차가 거의 정규분포를 따라야 한다.
> 3. **등분산성**(Constant variability): 최소제곱선 주위의 점들의 변동성이 대략 일정해야 한다.
> 4. **독립 관측치** (Independent observations): 관측치들이 서로 독립이어야 한다.

```python
# 그림 8.12: 조건 위반 예시
import matplotlib.pyplot as plt
import numpy as np

fig, axes = plt.subplots(2, 4, figsize=(16, 8))

np.random.seed(42)

# 1. 선형성 실패
x1 = np.linspace(0, 10, 50)
y1 = x1**2 - 10*x1 + 30 + np.random.normal(0, 3, 50)
axes[0, 0].scatter(x1, y1, alpha=0.7)
axes[0, 0].set_title('(a) 선형성 실패')
residuals1 = y1 - (np.polyval(np.polyfit(x1, y1, 1), x1))
axes[1, 0].scatter(x1, residuals1, alpha=0.7)
axes[1, 0].axhline(0, color='r', linestyle='--')
axes[1, 0].set_title('잔차에서 곡선 패턴')

# 2. 이상치 존재
x2 = np.random.uniform(0, 10, 49)
y2 = 2*x2 + 5 + np.random.normal(0, 2, 49)
x2 = np.append(x2, 5)
y2 = np.append(y2, 40)
axes[0, 1].scatter(x2, y2, alpha=0.7)
axes[0, 1].set_title('(b) 이상치 존재')
residuals2 = y2 - (np.polyval(np.polyfit(x2, y2, 1), x2))
axes[1, 1].scatter(x2, residuals2, alpha=0.7)
axes[1, 1].axhline(0, color='r', linestyle='--')
axes[1, 1].set_title('큰 잔차 1개')

# 3. 등분산성 실패 (팬 모양)
x3 = np.linspace(1, 10, 50)
y3 = 2*x3 + np.random.normal(0, 1, 50) * x3 * 0.5
axes[0, 2].scatter(x3, y3, alpha=0.7)
axes[0, 2].set_title('(c) 등분산성 실패')
residuals3 = y3 - (np.polyval(np.polyfit(x3, y3, 1), x3))
axes[1, 2].scatter(x3, residuals3, alpha=0.7)
axes[1, 2].axhline(0, color='r', linestyle='--')
axes[1, 2].set_title('팬 모양 패턴')

# 4. 시계열 데이터 (독립성 실패)
t = np.arange(50)
y4 = np.cumsum(np.random.normal(0, 1, 50)) + 0.5*t
axes[0, 3].scatter(t, y4, alpha=0.7)
axes[0, 3].set_title('(d) 독립성 실패 (시계열)')
residuals4 = y4 - (np.polyval(np.polyfit(t, y4, 1), t))
axes[1, 3].scatter(t, residuals4, alpha=0.7)
axes[1, 3].axhline(0, color='r', linestyle='--')
axes[1, 3].set_title('자기상관 패턴')

for ax in axes.flatten():
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('fig8_12_conditions.png', dpi=150, bbox_inches='tight')
plt.show()
```

---

### 8.2.4 최소제곱선 구하기

엘름허스트 데이터에 대해, 최소제곱 회귀선의 방정식을 다음과 같이 쓸 수 있다:
$$\widehat{\text{aid}} = \beta_0 + \beta_1 \times \text{family income}$$

모수는 관측 데이터를 사용하여 추정된다. 최소제곱선의 두 가지 성질:

> **최소제곱선 구하기**
>
> 1. **기울기 추정**:
> $$b_1 = \frac{s_y}{s_x} R$$
> 여기서 R은 상관계수, $s_x$와 $s_y$는 설명변수와 반응변수의 표본표준편차
>
> 2. **절편 추정** (직선이 ($\bar{x}$, $\bar{y}$)를 지남):
> $$b_0 = \bar{y} - b_1 \bar{x}$$

---

#### 예제 8.8

**문제**: 엘름허스트 대학 데이터에서 가족 소득과 장학금 데이터의 상관계수는 R = -0.499이다. 가족 소득의 표준편차는 $63,001이고, 장학금의 표준편차는 $5,456이다. 회귀선의 기울기를 계산하라.

**풀이**: 
$$b_1 = \frac{s_y}{s_x} R = \frac{5456}{63001} \times (-0.499) = -0.0431$$

---

#### 예제 8.9

**문제**: 장학금의 평균은 $19,939이고 가족 소득의 평균은 $101,784이다. 예제 8.8의 기울기를 사용하여 절편을 계산하라.

**풀이**: 
$$b_0 = \bar{y} - b_1 \bar{x} = 19939 - (-0.0431) \times 101784 = 19939 + 4387 = 24326$$

따라서 회귀 방정식은: $\widehat{\text{aid}} = 24,326 - 0.0431 \times \text{family income}$

```python
# 그림 8.15: 엘름허스트 대학 장학금 데이터
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(42)

# 엘름허스트 데이터 시뮬레이션
n = 50
family_income = np.random.lognormal(11.5, 0.5, n)  # 로그 정규분포
family_income = np.clip(family_income, 0, 250000)

aid = 24319 - 0.0431 * family_income + np.random.normal(0, 4500, n)
aid = np.clip(aid, 0, 40000)

plt.figure(figsize=(10, 7))
plt.scatter(family_income/1000, aid/1000, alpha=0.6, s=60)

# 회귀선
x_line = np.linspace(0, 250, 100)
y_line = (24319 - 0.0431 * x_line * 1000) / 1000
plt.plot(x_line, y_line, 'r-', linewidth=2, 
         label='$\\hat{y} = 24,319 - 0.0431x$')

plt.xlabel('가족 소득 (천 달러)', fontsize=12)
plt.ylabel('대학 장학금 (천 달러)', fontsize=12)
plt.title('엘름허스트 대학 신입생 50명의 장학금과 가족 소득', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.xlim(0, 260)
plt.ylim(0, 35)

plt.tight_layout()
plt.savefig('fig8_15_elmhurst.png', dpi=150, bbox_inches='tight')
plt.show()
```

---

### 8.2.5 회귀 모형 모수 추정치 해석

> **절편과 기울기 해석**
>
> **절편 (b₀)**: x = 0일 때 y의 예측값. 항상 의미 있는 것은 아님.
>
> **기울기 (b₁)**: x가 1단위 증가할 때 y의 예측된 평균 변화량.

#### 예제 8.10

**문제**: 모형 $\widehat{\text{aid}} = 24,319 - 0.0431 \times \text{family income}$에서 기울기를 해석하라.

**풀이**: 가족 소득이 $1,000 증가할 때마다, 장학금은 평균적으로 $43.10 감소할 것으로 예상된다.

---

### 8.2.6 외삽의 위험성

> **외삽** (Extrapolation)
>
> 원래 데이터의 범위 밖의 값에 모형 추정치를 적용하는 것을 **외삽**이라고 한다.
>
> 일반적으로, 선형 모형은 두 변수 사이의 실제 관계의 근사일 뿐이다. 외삽하면, 분석되지 않은 곳에서 근사적 선형 관계가 유효할 것이라는 신뢰할 수 없는 가정을 하게 된다.

#### 예제 8.14

**문제**: 모형 $\widehat{\text{aid}} = 24,319 - 0.0431 \times \text{family_income}$을 사용하여 가족 소득이 $1,000,000인 학생의 장학금을 추정하라.

**풀이**: 
$$\widehat{\text{aid}} = 24,319 - 0.0431 \times 1,000,000 = 24,319 - 43,100 = -18,781$$

모형은 이 학생이 -$18,781의 장학금을 받을 것으로 예측한다(!). 그러나 엘름허스트 대학은 일부 학생에게 등록금 위에 추가로 돈을 내라고 하는 음의 장학금을 제공하지 않는다. 이것이 **외삽의 위험성**이다.

---

### 8.2.7 R²를 사용하여 적합의 강도 설명하기

상관계수 R보다 R²(결정계수)를 사용하여 선형 적합의 강도를 설명하는 것이 더 일반적이다.

> **R²: 결정계수**
>
> R²는 **모형에 의해 설명되는 반응변수의 변동 비율**을 나타낸다.
>
> $$R^2 = 1 - \frac{s^2_{잔차}}{s^2_y} = 1 - \frac{Var(e_i)}{Var(y_i)}$$
>
> 단순 선형 회귀에서 R²는 상관계수 R의 제곱과 같다.

#### 예제: R² 계산 (엘름허스트 데이터)

장학금의 분산: $s^2_{aid} \approx 29,800,000$

잔차의 분산: $s^2_{RES} \approx 22,400,000$

$$R^2 = \frac{s^2_{aid} - s^2_{RES}}{s^2_{aid}} = \frac{29,800,000 - 22,400,000}{29,800,000} = \frac{7,500,000}{29,800,000} = 0.25$$

또는: $R = -0.499 \Rightarrow R^2 = (-0.499)^2 = 0.25$

**해석**: 장학금 변동의 약 25%가 가족 소득에 의해 설명된다.

---

#### 안내 실습 8.15

**문제**: 선형 모형이 상관관계 -0.97로 매우 강한 음의 관계를 갖는다면, 설명변수에 의해 설명되는 반응의 변동은 얼마인가?

**풀이**: $R^2 = (-0.97)^2 = 0.94$

약 94%의 변동이 선형 모형에 의해 설명된다.

---

### 8.2.8 두 수준의 범주형 예측변수

범주형 변수도 결과를 예측하는 데 유용하다. 닌텐도 Wii용 비디오 게임 마리오 카트의 eBay 경매를 고려하는데, 총 가격과 게임 상태("중고" 또는 "새 것")가 기록되었다.

범주를 회귀 방정식에 포함시키려면 수치 형태로 변환해야 한다. **지시변수(indicator variable)** cond new를 사용한다:

$$\text{cond new} = \begin{cases} 1 & \text{새 것이면} \\ 0 & \text{중고이면} \end{cases}$$


모형:
$$\widehat{\text{price}} = 42.87 + 10.90 \times \text{cond new}$$

#### 예제 8.16

**문제**: eBay 경매에서 마리오 카트 가격에 대한 모형의 모수를 해석하라.

**풀이**:
- **절편 (42.87)**: cond new = 0일 때, 즉 중고 게임의 평균 판매 가격은 $42.87
- **기울기 (10.90)**: 새 게임은 평균적으로 중고 게임보다 $10.90 더 비싸게 팔린다

---

## 8.2절 연습문제

### 연습문제 8.17: 회귀의 단위

**문제**: 성인 남성 표본에서 키(cm)로 체중(kg)을 예측하는 회귀를 고려하라. 상관계수, 절편, 기울기의 단위는 무엇인가?

**풀이**:
- 상관계수: **단위 없음** (무차원)
- 절편: **kg** (반응변수의 단위)
- 기울기: **kg/cm** (반응변수 단위 / 설명변수 단위)

---

### 연습문제 8.19: 과대/과소 추정

**문제**: 사과의 무게로 유통기한을 예측하는 회귀선을 적합했다. 특정 사과에 대해 유통기한을 4.6일로 예측했고 잔차는 -0.6일이다. 유통기한을 과대추정했는가 과소추정했는가?

**풀이**:
- 잔차 = 실제값 - 예측값 = -0.6
- 실제 유통기한 = 4.6 + (-0.6) = 4.0일
- 실제값(4.0) < 예측값(4.6)이므로 **과대추정**

---

### 연습문제 8.23: Coast Starlight, 파트 II

**문제**: Coast Starlight 기차의 이동 시간과 거리 데이터가 있다.
- 평균 이동 시간: 129분, 표준편차: 113분
- 평균 거리: 108마일, 표준편차: 99마일
- 상관계수: 0.636

(a) 이동 시간을 예측하는 회귀선 방정식을 작성하라.

**풀이**:

$$b_1 = R \times \frac{s_y}{s_x} = 0.636 \times \frac{113}{99} = 0.726$$

$$b_0 = \bar{y} - b_1 \times \bar{x} = 129 - 0.726 \times 108 = 50.6$$

**회귀선**: $\widehat{\text{이동시간}} = 50.6 + 0.726 \times \text{거리}$

---

### 연습문제 8.25: 살인과 빈곤, 파트 I

**회귀 출력**:

| | Estimate | Std. Error | t value | Pr(>|t|) |
|---|----------|------------|---------|----------|
| (Intercept) | -29.901 | 7.789 | -3.839 | 0.001 |
| poverty% | 2.559 | 0.390 | 6.562 | 0.000 |

s = 5.512, R² = 70.52%

**풀이**:

(a) 선형 모형: $\widehat{\text{살인율}} = -29.901 + 2.559 \times \text{빈곤율\%}$

(b) 절편: 빈곤율이 0%일 때 살인율은 -29.901 (의미 없음, 음수 불가능)

(c) 기울기: 빈곤율이 1% 포인트 높아질 때 살인율이 2.559건 증가

(d) R²: 살인율 변동의 70.52%가 빈곤율에 의해 설명됨

(e) 상관계수: $R = \sqrt{0.7052} = 0.84$ (기울기가 양수이므로 양수)

---

> **새로운 시각: 회귀분석의 실용적 적용**
>
> 회귀분석을 실제로 적용할 때 고려해야 할 사항:
>
> 1. **탐색적 데이터 분석 먼저**: 회귀 전에 항상 산점도를 그려 관계를 시각적으로 확인
> 2. **조건 검증**: 4가지 조건(선형성, 정규성, 등분산성, 독립성)을 항상 확인
> 3. **외삽 주의**: 데이터 범위 밖의 예측은 신뢰할 수 없음
> 4. **인과관계 주의**: 상관관계가 인과관계를 의미하지 않음
> 5. **R² 해석 주의**: R²가 높아도 모형이 유용하지 않을 수 있고, R²가 낮아도 유용할 수 있음

## 8.3 선형 회귀에서의 이상치 유형

이 절에서는 어떤 이상치가 중요하고 영향력이 있는지 결정하는 기준을 식별한다.

회귀에서 **이상치(outliers)**는 점들의 구름에서 멀리 떨어진 관측치이다. 이 점들은 최소제곱선에 강한 영향을 미칠 수 있으므로 특히 중요하다.

### 이상치의 세 가지 유형

> **이상치 유형 정의**
>
> 1. **높은 지렛대 점 (High leverage point)**: 설명변수(x) 방향으로 데이터의 중심에서 멀리 떨어진 점. 회귀선의 기울기에 큰 영향을 미칠 *가능성*이 있다.
>
> 2. **영향력 있는 점 (Influential point)**: 실제로 회귀선의 기울기에 상당한 영향을 미치는 점. 이 점을 제거하면 회귀선이 크게 변한다.
>
> 3. **이상치 (Outlier)**: 다른 점들의 패턴에서 벗어난 점. 회귀선에서 수직으로 멀리 떨어져 있다.

---

## 8.4 선형 회귀에 대한 추론

### 8.4.1 중간선거와 실업률

미국 하원 의원 선거는 2년마다 열리며, 4년마다 미국 대통령 선거와 일치한다. 대통령 임기 중간에 열리는 하원 선거를 **중간선거(midterm elections)**라고 한다.

한 정치 이론은 실업률이 높을수록 대통령 소속 정당이 중간선거에서 더 나쁜 성적을 낼 것이라고 제안한다.

1898년부터 2018년까지의 중간선거 데이터(대공황 제외)를 사용한 회귀선:

$$\text{대통령 소속당 하원 의석 \% 변화} = -7.36 - 0.89 \times (\text{실업률})$$

### 가설 검정 설정

직선에 음의 기울기가 있지만, 이것이 우연일 수 있는지 궁금하다:

> **H₀**: β₁ = 0 (참 선형 모형의 기울기는 0)
>
> **Hₐ**: β₁ ≠ 0 (참 선형 모형의 기울기는 0이 아님)

### 8.4.2 소프트웨어의 회귀 출력 이해하기

통계 소프트웨어는 회귀 분석의 상세한 결과를 제공한다:

| | Estimate | Std. Error | t value | Pr(>|t|) |
|---|----------|------------|---------|----------|
| (Intercept) | -7.3644 | 5.1553 | -1.43 | 0.1646 |
| unemp | -0.8897 | 0.8350 | -1.07 | 0.2961 |

df = 27

**검정 통계량 계산**:
$$T = \frac{\text{추정치} - \text{귀무값}}{SE} = \frac{-0.8897 - 0}{0.8350} = -1.07$$

p-값 = 0.2961 > 0.05이므로 H₀를 기각하지 못한다.

### 8.4.3 계수에 대한 신뢰구간

> **계수에 대한 신뢰구간**
>
> $$b_i \pm t^*_{df} \times SE_{b_i}$$

**예제**: 엘름허스트 데이터에서 가족 소득 계수의 95% 신뢰구간

- 점추정치: -0.0431
- 표준오차: SE = 0.0108
- df = 48, t*₄₈ = 2.01

$$-0.0431 \pm 2.01 \times 0.0108 = (-0.0648, -0.0214)$$

**해석**: 가족 소득이 $1 증가할 때마다, 장학금은 평균적으로 $0.0214에서 $0.0648 감소할 것으로 95% 신뢰한다.

---

## 8.4절 연습문제 풀이

### 연습문제 8.31: 신체 측정, 파트 IV

**회귀 출력** (키로 체중 예측, 507명):

| | Estimate | Std. Error | t value | Pr(>|t|) |
|---|----------|------------|---------|----------|
| (Intercept) | -105.0113 | 7.5394 | -13.93 | 0.0000 |
| height | 1.0176 | 0.0440 | 23.13 | 0.0000 |

**풀이**:
(a) 키와 체중 사이에 양의, 강한 선형 관계가 있다.
(b) 회귀선: ŷ = -105.01 + 1.02×키. 기울기: 키 1cm 증가 시 체중 약 1.02kg 증가
(c) p-값 < 0.05이므로 H₀ 기각. 강력한 증거 있음.
(d) R² = 0.72² = 0.52 (체중 변동의 52%가 키에 의해 설명됨)

### 연습문제 8.35: 살인과 빈곤, 파트 II

(c) **95% 신뢰구간** (n=20, df=18, t*₁₈ = 2.10):
$$2.559 \pm 2.10 \times 0.390 = (1.74, 3.38)$$

(d) 가설 검정(H₀ 기각)과 신뢰구간(0 미포함)의 결과가 일관된다.

### 연습문제 8.37: 참/거짓

(a) 상관계수 -0.90 vs 0.5: **참** (|-0.90| > |0.5|)
(b) 상관관계는 두 변수 간의 연관성 측도: **거짓** (수치형 변수의 선형 연관성만)

---

> **새로운 시각: 회귀분석의 실용적 적용**
>
> 회귀분석을 실제로 적용할 때 고려해야 할 사항:
>
> 1. **탐색적 데이터 분석 먼저**: 회귀 전에 항상 산점도를 그려 관계를 시각적으로 확인
> 2. **조건 검증**: 4가지 조건(선형성, 정규성, 등분산성, 독립성)을 항상 확인
> 3. **외삽 주의**: 데이터 범위 밖의 예측은 신뢰할 수 없음
> 4. **인과관계 주의**: 상관관계가 인과관계를 의미하지 않음
> 5. **R² 해석 주의**: R²가 높아도 모형이 유용하지 않을 수 있고, R²가 낮아도 유용할 수 있음

